{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "\n",
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import glob\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing the training set\n",
    "fruit_images = []\n",
    "labels = [] \n",
    "for fruit_dir_path in glob.glob(\"./fruits-360/Training/*\"):\n",
    "    fruit_label = fruit_dir_path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.jpg\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        image = cv2.resize(image, (45, 45))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        fruit_images.append(image)\n",
    "        labels.append(fruit_label)\n",
    "fruit_images = np.array(fruit_images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique numbers so that I have a proper list of the folders.\n",
    "label_to_id_dict = {v:i for i,v in enumerate(np.unique(labels))}\n",
    "id_to_label_dict = {v: k for k, v in label_to_id_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Apple Braeburn',\n",
       " 1: 'Apple Golden 1',\n",
       " 2: 'Apple Golden 2',\n",
       " 3: 'Apple Golden 3',\n",
       " 4: 'Apple Granny Smith',\n",
       " 5: 'Apple Red 1',\n",
       " 6: 'Apple Red 2',\n",
       " 7: 'Apple Red 3',\n",
       " 8: 'Apple Red Delicious',\n",
       " 9: 'Apple Red Yellow',\n",
       " 10: 'Apricot',\n",
       " 11: 'Avocado',\n",
       " 12: 'Avocado ripe',\n",
       " 13: 'Banana',\n",
       " 14: 'Banana Red',\n",
       " 15: 'Cactus fruit',\n",
       " 16: 'Cantaloupe 1',\n",
       " 17: 'Cantaloupe 2',\n",
       " 18: 'Carambula',\n",
       " 19: 'Cherry 1',\n",
       " 20: 'Cherry 2',\n",
       " 21: 'Cherry Rainier',\n",
       " 22: 'Cherry Wax Black',\n",
       " 23: 'Cherry Wax Red',\n",
       " 24: 'Cherry Wax Yellow',\n",
       " 25: 'Clementine',\n",
       " 26: 'Cocos',\n",
       " 27: 'Dates',\n",
       " 28: 'Granadilla',\n",
       " 29: 'Grape Pink',\n",
       " 30: 'Grape White',\n",
       " 31: 'Grape White 2',\n",
       " 32: 'Grapefruit Pink',\n",
       " 33: 'Grapefruit White',\n",
       " 34: 'Guava',\n",
       " 35: 'Huckleberry',\n",
       " 36: 'Kaki',\n",
       " 37: 'Kiwi',\n",
       " 38: 'Kumquats',\n",
       " 39: 'Lemon',\n",
       " 40: 'Lemon Meyer',\n",
       " 41: 'Limes',\n",
       " 42: 'Lychee',\n",
       " 43: 'Mandarine',\n",
       " 44: 'Mango',\n",
       " 45: 'Maracuja',\n",
       " 46: 'Melon Piel de Sapo',\n",
       " 47: 'Mulberry',\n",
       " 48: 'Nectarine',\n",
       " 49: 'Orange',\n",
       " 50: 'Papaya',\n",
       " 51: 'Passion Fruit',\n",
       " 52: 'Peach',\n",
       " 53: 'Peach Flat',\n",
       " 54: 'Pear',\n",
       " 55: 'Pear Abate',\n",
       " 56: 'Pear Monster',\n",
       " 57: 'Pear Williams',\n",
       " 58: 'Pepino',\n",
       " 59: 'Physalis',\n",
       " 60: 'Physalis with Husk',\n",
       " 61: 'Pineapple',\n",
       " 62: 'Pineapple Mini',\n",
       " 63: 'Pitahaya Red',\n",
       " 64: 'Plum',\n",
       " 65: 'Pomegranate',\n",
       " 66: 'Quince',\n",
       " 67: 'Rambutan',\n",
       " 68: 'Raspberry',\n",
       " 69: 'Salak',\n",
       " 70: 'Strawberry',\n",
       " 71: 'Strawberry Wedge',\n",
       " 72: 'Tamarillo',\n",
       " 73: 'Tangelo',\n",
       " 74: 'Walnut'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look to see if I have the folders imported \n",
    "id_to_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn labels into numerical arrays.\n",
    "label_ids = np.array([label_to_id_dict[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37836, 45, 45, 3), (37836,), (37836,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at data shape\n",
    "fruit_images.shape, label_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the testing set.\n",
    "validation_fruit_images = []\n",
    "validation_labels = [] \n",
    "for fruit_dir_path in glob.glob(\"./fruits-360/Test/*\"):\n",
    "    fruit_label = fruit_dir_path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.jpg\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        image = cv2.resize(image, (45, 45))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        validation_fruit_images.append(image)\n",
    "        validation_labels.append(fruit_label)\n",
    "validation_fruit_images = np.array(validation_fruit_images)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_label_ids = np.array([label_to_id_dict[x] for x in validation_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12709, 45, 45, 3), (12709,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_fruit_images.shape, validation_label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes: (37836, 45, 45, 3) (12709, 45, 45, 3) (37836, 75) (12709, 75)\n",
      "Flattened: (37836, 6075) (12709, 6075)\n"
     ]
    }
   ],
   "source": [
    "# Create the train test split.\n",
    "X_train, X_test = fruit_images, validation_fruit_images\n",
    "Y_train, Y_test = label_ids, validation_label_ids\n",
    "\n",
    "#Normalize color values to between 0 and 1\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "#Reshaping to a flattened version\n",
    "X_flat_train = X_train.reshape(X_train.shape[0], 45*45*3)\n",
    "X_flat_test = X_test.reshape(X_test.shape[0], 45*45*3)\n",
    "\n",
    "#Encoding the output\n",
    "Y_train = keras.utils.to_categorical(Y_train, 75)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 75)\n",
    "\n",
    "print('Original Sizes:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "print('Flattened:', X_flat_train.shape, X_flat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 45, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "plt.imshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               777728    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 75)                4875      \n",
      "=================================================================\n",
      "Total params: 790,859\n",
      "Trainable params: 790,859\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 37836 samples, validate on 12709 samples\n",
      "Epoch 1/10\n",
      "37836/37836 [==============================] - 5s 129us/step - loss: 3.3211 - acc: 0.1720 - val_loss: 1.7661 - val_acc: 0.5063\n",
      "Epoch 2/10\n",
      "37836/37836 [==============================] - 5s 122us/step - loss: 1.7125 - acc: 0.4750 - val_loss: 1.0102 - val_acc: 0.7145\n",
      "Epoch 3/10\n",
      "37836/37836 [==============================] - 5s 123us/step - loss: 1.2453 - acc: 0.6086 - val_loss: 0.8693 - val_acc: 0.7390\n",
      "Epoch 4/10\n",
      "37836/37836 [==============================] - 5s 124us/step - loss: 1.0112 - acc: 0.6782 - val_loss: 0.6694 - val_acc: 0.8104\n",
      "Epoch 5/10\n",
      "37836/37836 [==============================] - 5s 123us/step - loss: 0.8718 - acc: 0.7214 - val_loss: 0.6635 - val_acc: 0.7957\n",
      "Epoch 6/10\n",
      "37836/37836 [==============================] - 5s 122us/step - loss: 0.7744 - acc: 0.7521 - val_loss: 0.4851 - val_acc: 0.8357\n",
      "Epoch 7/10\n",
      "37836/37836 [==============================] - 5s 124us/step - loss: 0.7033 - acc: 0.7762 - val_loss: 0.5723 - val_acc: 0.8149\n",
      "Epoch 8/10\n",
      "37836/37836 [==============================] - 5s 123us/step - loss: 0.6485 - acc: 0.7926 - val_loss: 0.5732 - val_acc: 0.8200\n",
      "Epoch 9/10\n",
      "37836/37836 [==============================] - 5s 123us/step - loss: 0.6147 - acc: 0.8053 - val_loss: 0.5633 - val_acc: 0.8252\n",
      "Epoch 10/10\n",
      "37836/37836 [==============================] - 5s 123us/step - loss: 0.5735 - acc: 0.8171 - val_loss: 0.5105 - val_acc: 0.8346\n",
      "Test loss: 0.5105185561382671\n",
      "Test accuracy: 0.8346053977507988\n"
     ]
    }
   ],
   "source": [
    "# 1. MLP Model\n",
    "model_dense = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model_dense.add(Dense(128, activation='relu', input_shape=(X_flat_train.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model_dense.add(Dropout(0.1))\n",
    "model_dense.add(Dense(64, activation='relu'))\n",
    "model_dense.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model_dense.add(Dense(75, activation='softmax'))\n",
    "\n",
    "model_dense.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model_dense.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_dense = model_dense.fit(X_flat_train, Y_train,\n",
    "                          batch_size=128,\n",
    "                          epochs=10,\n",
    "                          verbose=1,\n",
    "                          validation_data=(X_flat_test, Y_test))\n",
    "score = model_dense.evaluate(X_flat_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 256)               1555456   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 75)                9675      \n",
      "=================================================================\n",
      "Total params: 1,647,563\n",
      "Trainable params: 1,647,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 37836 samples, validate on 12709 samples\n",
      "Epoch 1/10\n",
      "37836/37836 [==============================] - 10s 261us/step - loss: 2.9270 - acc: 0.2102 - val_loss: 1.4326 - val_acc: 0.5807\n",
      "Epoch 2/10\n",
      "37836/37836 [==============================] - 9s 234us/step - loss: 1.3162 - acc: 0.5672 - val_loss: 0.7732 - val_acc: 0.7439\n",
      "Epoch 3/10\n",
      "37836/37836 [==============================] - 9s 237us/step - loss: 0.8696 - acc: 0.7122 - val_loss: 0.7343 - val_acc: 0.7516\n",
      "Epoch 4/10\n",
      "37836/37836 [==============================] - 10s 252us/step - loss: 0.6313 - acc: 0.7908 - val_loss: 0.4279 - val_acc: 0.8605\n",
      "Epoch 5/10\n",
      "37836/37836 [==============================] - 9s 250us/step - loss: 0.5015 - acc: 0.8361 - val_loss: 0.3612 - val_acc: 0.8728\n",
      "Epoch 6/10\n",
      "37836/37836 [==============================] - 9s 243us/step - loss: 0.4034 - acc: 0.8656 - val_loss: 0.5238 - val_acc: 0.8289\n",
      "Epoch 7/10\n",
      "37836/37836 [==============================] - 9s 236us/step - loss: 0.3516 - acc: 0.8856 - val_loss: 0.3850 - val_acc: 0.8853\n",
      "Epoch 8/10\n",
      "37836/37836 [==============================] - 9s 246us/step - loss: 0.2944 - acc: 0.9018 - val_loss: 1.0337 - val_acc: 0.7559\n",
      "Epoch 9/10\n",
      "37836/37836 [==============================] - 9s 241us/step - loss: 0.2687 - acc: 0.9156 - val_loss: 0.3468 - val_acc: 0.9016\n",
      "Epoch 10/10\n",
      "37836/37836 [==============================] - 9s 233us/step - loss: 0.2468 - acc: 0.9237 - val_loss: 0.2899 - val_acc: 0.9101\n",
      "Test loss: 0.28993584296811875\n",
      "Test accuracy: 0.9101424187583602\n"
     ]
    }
   ],
   "source": [
    "# Going to add more dense layers.\n",
    "model_deep = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model_deep.add(Dense(256, activation='relu', input_shape=(X_flat_train.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "model_deep.add(Dense(128, activation='relu'))\n",
    "model_deep.add(Dropout(0.05))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model_deep.add(Dense(75, activation='softmax'))\n",
    "\n",
    "model_deep.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model_deep.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history_deep = model_deep.fit(X_flat_train, Y_train,\n",
    "                          batch_size=128,\n",
    "                          epochs=10,\n",
    "                          verbose=1,\n",
    "                          validation_data=(X_flat_test, Y_test))\n",
    "score = model_deep.evaluate(X_flat_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more layers increased accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37836 samples, validate on 12709 samples\n",
      "Epoch 1/5\n",
      "37836/37836 [==============================] - 184s 5ms/step - loss: 2.4603 - acc: 0.3573 - val_loss: 0.5903 - val_acc: 0.8295\n",
      "Epoch 2/5\n",
      "37836/37836 [==============================] - 182s 5ms/step - loss: 0.4676 - acc: 0.8495 - val_loss: 0.2469 - val_acc: 0.9280\n",
      "Epoch 3/5\n",
      "37836/37836 [==============================] - 188s 5ms/step - loss: 0.1940 - acc: 0.9340 - val_loss: 0.1702 - val_acc: 0.9429\n",
      "Epoch 4/5\n",
      "37836/37836 [==============================] - 184s 5ms/step - loss: 0.1229 - acc: 0.9568 - val_loss: 0.1744 - val_acc: 0.9417\n",
      "Epoch 5/5\n",
      "37836/37836 [==============================] - 176s 5ms/step - loss: 0.0944 - acc: 0.9663 - val_loss: 0.1656 - val_acc: 0.9518\n",
      "Test loss: 0.16563410499914552\n",
      "Test accuracy: 0.951766464710048\n"
     ]
    }
   ],
   "source": [
    "# 3. Convolutional Neural Network\n",
    "model_cnn = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(45, 45, 3)))\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(75, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_cnn.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only 5 epochs, I was able to obtain 95% test accuracy. I will increase it to 7 epochs, but that will probably overfit the data even more so I will decrease the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37836 samples, validate on 12709 samples\n",
      "Epoch 1/7\n",
      "37836/37836 [==============================] - 186s 5ms/step - loss: 0.0800 - acc: 0.9705 - val_loss: 0.1830 - val_acc: 0.9467\n",
      "Epoch 2/7\n",
      "37836/37836 [==============================] - 185s 5ms/step - loss: 0.0648 - acc: 0.9754 - val_loss: 0.1450 - val_acc: 0.9644\n",
      "Epoch 3/7\n",
      "37836/37836 [==============================] - 187s 5ms/step - loss: 0.0610 - acc: 0.9774 - val_loss: 0.1116 - val_acc: 0.9655\n",
      "Epoch 4/7\n",
      "37836/37836 [==============================] - 186s 5ms/step - loss: 0.0525 - acc: 0.9801 - val_loss: 0.1014 - val_acc: 0.9688\n",
      "Epoch 5/7\n",
      "37836/37836 [==============================] - 194s 5ms/step - loss: 0.0466 - acc: 0.9817 - val_loss: 0.1407 - val_acc: 0.9644\n",
      "Epoch 6/7\n",
      "37836/37836 [==============================] - 188s 5ms/step - loss: 0.0442 - acc: 0.9821 - val_loss: 0.1210 - val_acc: 0.9711\n",
      "Epoch 7/7\n",
      "37836/37836 [==============================] - 187s 5ms/step - loss: 0.0431 - acc: 0.9824 - val_loss: 0.1271 - val_acc: 0.9692\n",
      "Test loss: 0.12713746363177464\n",
      "Test accuracy: 0.9692344008183177\n"
     ]
    }
   ],
   "source": [
    "# 4. Modified CNN\n",
    "model_cnn.fit(X_train, Y_train,\n",
    "          batch_size=100,\n",
    "          epochs=7,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I messed up on this one since it's accuracy started off so high. I forgot to reset it so it may have taken the previous results into account. I did notice that the accuracy did not improve by much even though I increased the epoch size which may be due to the fact I lowered the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (37836, 45, 45, 3)\n",
      "37836 train samples\n",
      "12709 test samples\n",
      "Train on 37836 samples, validate on 12709 samples\n",
      "Epoch 1/5\n",
      "37836/37836 [==============================] - 152s 4ms/step - loss: 3.7393 - acc: 0.0695 - val_loss: 3.2530 - val_acc: 0.1089\n",
      "Epoch 2/5\n",
      "37836/37836 [==============================] - 151s 4ms/step - loss: 2.8792 - acc: 0.2092 - val_loss: 2.8747 - val_acc: 0.1981\n",
      "Epoch 3/5\n",
      "37836/37836 [==============================] - 150s 4ms/step - loss: 2.2893 - acc: 0.3607 - val_loss: 2.3743 - val_acc: 0.3013\n",
      "Epoch 4/5\n",
      "37836/37836 [==============================] - 147s 4ms/step - loss: 1.8340 - acc: 0.4810 - val_loss: 1.9605 - val_acc: 0.3837\n",
      "Epoch 5/5\n",
      "37836/37836 [==============================] - 147s 4ms/step - loss: 1.4651 - acc: 0.5916 - val_loss: 1.5169 - val_acc: 0.5684\n",
      "Test loss: 1.5168628042411518\n",
      "Test accuracy: 0.5684160830907231\n"
     ]
    }
   ],
   "source": [
    "# 5. Heirarchical Recurrent Neural Network\n",
    "\n",
    "# Set up the training and test set again.\n",
    "x_train = fruit_images\n",
    "x_test = validation_fruit_images\n",
    "y_train = label_ids\n",
    "y_test = validation_label_ids\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 75\n",
    "epochs = 5\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "x_train = x_train.reshape(x_train.shape[0], 45, 45, 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 45, 45, 3)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Converts class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After running the 5 different models, the MLP model was the quickest to run and was highly accurate. Perhaps if I increased the epoch size and the batch size even more it would be even more accurate. A definite way to increase the accuracy of the regular MLP model would be to increase the amount of layers.  \n",
    "\n",
    "The Convolutional Neural Network was extremely accurate, but it took approximately 3 minutes for each epoch to run. I fear that if I increased the epoch size, it would overfit the model which would be bad. This model has a chance of being in the front if the computer specs are up to par. \n",
    "\n",
    "Lastly, the Heirarchical Recurrent Neural Network took about as long as the CNN, but the accuracy was gravely different from its counterpart. I'm surprised it's accuracy is as low as it is, given how the example problem had such a high score. I even increased it epoch size to the same as the CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
