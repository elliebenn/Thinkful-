{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at Amazon's robots.txt file (or Twitter's, or Facebook's), you may be surprised to see them prohibit or severely restrict scraping.  Aren't there a lot of projects online using Twitter data?  And how dare they keep all that delicious, delicious information to themselves?  But before you start setting `'ROBOTSTXT_OBEY' = False`, read on!\n",
    "\n",
    "Most of The Big Websites (Google, Facebook, Twitter, etc) have APIs that allow you to access their information programmatically without using webpages.  This is good for both you and the website.  With an API, you can ask the server to send you only the specific information you want, without having to retrieve, filter out, and discard the CSS, HTML, PHP, and other code from the website.  This minimizes demand on the server and speeds up your task.  \n",
    "\n",
    "APIs typically include their own throttling to keep you from overloading the server, usually done by limiting the number of server requests per hour to a certain number.  \n",
    "\n",
    "To access an API, you will usually need an API key or token that uniquely identifies you.  This lets the company or service providing the API keep an eye on your usage and track what you are doing.  Different API keys can also be associated with different levels of authorization and access, so they work as a data security measure.  Keys or tokens may also be set to expire after a certain amount of time or number of uses.\n",
    "\n",
    "## Anatomy of an API\n",
    "\n",
    "*Access*- You request a key.  Your program provides the key with each API call, and it determines what your program can do in the API.  \n",
    "*Requests*- Your program requests the data you want with a call to the API.  The request will be made up of a method (type of query, using language defined by the API) and parameters (refine the query).  \n",
    "*Response*- The data returned by the API, usually in a common format such as JSON that your program can parse.  \n",
    "\n",
    "The specific syntax for each of these elements, and the format of the response, will vary from API to API.  In addition, APIs vary widely in their level of documentation and ease of use.  Before diving too deeply into an API-scraping project, do some judicious googling and if you see a lot of posts [like this one](https://mollyrocket.com/casey/stream_0029.html) consider going elsewhere.  Not all websites put their APIs front-and-center (did you know there are APIs for [NASA](https://api.nasa.gov/), [Marvel Comics](http://developer.marvel.com/), and [Star Wars](https://swapi.co/)?) so google will be your friend there as well.\n",
    "\n",
    "## Basics of API Queries: Wikipedia's API\n",
    "\n",
    "The process of using an API sounds a lot like scraping (make request, get response), but with an occasional added authorization layer.  Scrapy can handle authorization, so we can use it to access APIs too.\n",
    "\n",
    "That said, the first API we'll pull from is [Wikipedia's](https://www.mediawiki.org/wiki/API:Main_page), which doesn't require an authorization key.  Aside from needing to master the API's language, you'll find that using scrapy with an API is very similar to using scrapy on a website.\n",
    "\n",
    "We want to know what other entries on Wikipedia link to the [Monty Python](https://en.wikipedia.org/wiki/Monty_Python) page.  To do this, we can build a query using the [Wikipedia API Sandbox](https://en.wikipedia.org/wiki/Special:ApiSandbox).  Someone who is comfortable with the MediaWiki API syntax wouldn't need to use the sandbox, but for beginners it is very handy.  Note that API queries are nothing like SQL queries in syntax, despite their shared name.\n",
    "\n",
    "The query we will use looks like this:\n",
    "`https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect`\n",
    "\n",
    "Let's break that down into it's components:\n",
    "\n",
    "* `w/api.php`\n",
    "    * Tells the server that we are using the API to pull info, rather than scraping the raw pages.  \n",
    "    \n",
    "* `action=query`   \n",
    "    * We want information from the API (as opposed to changing information in the API)  \n",
    "    \n",
    "* `format=xml`  \n",
    "    * Format the return in xml- then we will parse it with xpath  \n",
    "    \n",
    "* `prop=linkshere`  \n",
    "    * We are interested in which pages link to our target page \n",
    "    \n",
    "* `titles=Monty_Python`  \n",
    "    * The target page is the Monty Python page.  Note that we used the exact name of the wikipedia page (Monty_Python).  \n",
    "    \n",
    "* `lhprop=title`  \n",
    "    * From those links, we want the title of each page  \n",
    "    \n",
    "* `redirect`  \n",
    "    * We also want to know if that link is a redirect  \n",
    "    \n",
    "\n",
    "The syntax of the MediaWiki API is based on php, thus the inclusion of `?` and `&` in the query.\n",
    "\n",
    "For most of the query elements, we could have passed multiple arguments.  For example, we could request the URL as well as the title of the linking pages, or asked for all the pages that link to Monty_Python and to Monty_Python's_Flying_Circus.  \n",
    "\n",
    "A query like this highlights why APIs are so handy.  Without an API, to find out the name of every page on Wikipedia that links to the Monty Python page we would have to scrape every single one of the 5,000,000+ articles in the English-language Wikipedia.  \n",
    "\n",
    "If you haven't done so already, click on the query link above and see what it returns.\n",
    "\n",
    "\n",
    "\n",
    "## Why use Scrapy for API calls\n",
    "\n",
    "For some API calls, scrapy would be overkill.  If you know that your query can be answered in one response, then you don't need scrapy- you can use the `requests` library to make your API call and a library like `lxml` to parse the return.\n",
    "\n",
    "The Wikipedia API, however, will only return ten items at a time in response to a query.  This sort of limitation is common to APIs to avoid overwhelming the server.  We can use scrapy to iterate over query results the same way that we iterated over the pages of the EverydaySexism website. \n",
    "\n",
    "Let's see the Wikipedia API and scrapy in action:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 links extracted!\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = \"WS\"\n",
    "    \n",
    "    # Here is where we insert our API call.\n",
    "    start_urls = [\n",
    "        'https://en.wikipedia.org/w/api.php?action=query&format=xml&prop=linkshere&titles=Monty_Python&lhprop=title%7Credirect'\n",
    "        ]\n",
    "\n",
    "    # Identifying the information we want from the query response and extracting it using xpath.\n",
    "    def parse(self, response):\n",
    "        for item in response.xpath('//lh'):\n",
    "            # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "            # Other codes indicate links from 'Talk' pages, etc.  Since we are only interested in entries, we filter:\n",
    "            if item.xpath('@ns').extract_first() == '0':\n",
    "                yield {\n",
    "                    'title': item.xpath('@title').extract_first() \n",
    "                    }\n",
    "        # Getting the information needed to continue to the next ten entries.\n",
    "        next_page = response.xpath('continue/@lhcontinue').extract_first()\n",
    "        \n",
    "        # Recursively calling the spider to process the next ten entries, if they exist.\n",
    "        if next_page is not None:\n",
    "            next_page = '{}&lhcontinue={}'.format(self.start_urls[0],next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'PythonLinks.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False,\n",
    "    # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "    'CLOSESPIDER_PAGECOUNT' : 10\n",
    "})\n",
    "                                         \n",
    "\n",
    "# Starting the crawler with our spider.\n",
    "process.crawl(WikiSpider)\n",
    "process.start()\n",
    "print('First 100 links extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 1)\n",
      "                    title\n",
      "89  Surrealist automatism\n",
      "90        Raymond Queneau\n",
      "91           Andr√© Breton\n",
      "92      Tim Brooke-Taylor\n",
      "93           Fifth Beatle\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Checking whether we got data \n",
    "\n",
    "Monty=pd.read_json('PythonLinks.json', orient='records')\n",
    "print(Monty.shape)\n",
    "print(Monty.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "Our API call was successful.  While we examined 100 links, we only saved 92 (the others weren't links from entry pages).  \n",
    "\n",
    "We've barely scraped (pun intended) the surface of what scrapy and APIs can do.  Scrapy has changed a lot in the years since its debut, so when googling make sure the answers you see are from 2015 at the latest-- otherwise you'll likely not be able to use the code.  \n",
    "\n",
    "Back to the issue of authorization keys- often the key is simply included in the query string as an additional arguments.  In other cases, if you need your scraper to be able to enter a key or login information into a form, scrapy [has you covered](http://stackoverflow.com/questions/30102199/form-authentication-login-a-site-using-scrapy).  \n",
    "\n",
    "There's a lot of fun to be had in scraping and APIs-- it's a way to feel like you're getting a lot of information with very little effort!  Beware, however.  You're not getting information at all.  Scraping gives you *data*, an undifferentiated mess of bytes with no compelling meaning on its own.  Think of that list of Wiki entries that link to Monty Python.  It's cool that we could get it, but what does it mean?  Your job as a data scientist is to convert *data* to *information*-- something people can use to make decisions or understand the world.  Modeling data to get information is hard but worthwhile work, and its those kinds of projects that will really build your portfolio as you go on the market.  \n",
    "\n",
    "That said, scraping up some original data can provide the *foundation* for an interesting and original final project.\n",
    "\n",
    "## Challenge\n",
    "\n",
    "Do a little scraping or API-calling of your own.  Pick a new website and see what you can get out of it.  Expect that you'll run into bugs and blind alleys, and rely on your mentor to help you get through.  \n",
    "\n",
    "Formally, your goal is to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "2) Iterate over multiple pages/queries  \n",
    "3) Save the data to your computer  \n",
    "\n",
    "Once you have your data, compute some statistical summaries and/or visualizations that give you some new insights into your scraping topic of interest.  Write up a report from scraping code to summary and share it with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class CLSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"CL\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://sfbay.craigslist.org/search/sss?query=wilson%20blade&sort=rel',\n",
    "    ]\n",
    "\n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for posting in response.xpath('//p'):\n",
    "            \n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'title': posting.xpath('a[@class=\"result-title hdrlnk\"]/text()').extract_first(),\n",
    "                'date': posting.xpath('time[@class=\"result-date\"]/text()').extract_first(),\n",
    "                'price': posting.xpath('span/span[@class=\"result-price\"]/text()').extract_first()\n",
    "            }\n",
    "        next_page = response.xpath('//div/div/span[@class=\"buttons\"]/a[3][@href]').extract_first()\n",
    "        \n",
    "        pagenum = int(re.findall(r'\\d+',next_page)[0])\n",
    "        \n",
    "        if next_page is not None and pagenum < 10:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'CLdata.json',  # Name our storage file.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcamp_Kevin (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(CLSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jul 28</td>\n",
       "      <td>$100</td>\n",
       "      <td>Wilson BLX Blade Tour New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul 28</td>\n",
       "      <td>$125</td>\n",
       "      <td>Wilson BLX Blade Lite \"NEW\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jul 27</td>\n",
       "      <td>$45</td>\n",
       "      <td>Blade Wilson [K] Factor Tennis Racket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jul 30</td>\n",
       "      <td>$110</td>\n",
       "      <td>Wilson Tennis Rackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jul 25</td>\n",
       "      <td>$160</td>\n",
       "      <td>Wilson D200 RH Irons - 4 Iron to Pitching Wedg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date price                                              title\n",
       "0  Jul 28  $100                          Wilson BLX Blade Tour New\n",
       "1  Jul 28  $125                        Wilson BLX Blade Lite \"NEW\"\n",
       "2  Jul 27   $45              Blade Wilson [K] Factor Tennis Racket\n",
       "3  Jul 30  $110                              Wilson Tennis Rackets\n",
       "4  Jul 25  $160  Wilson D200 RH Irons - 4 Iron to Pitching Wedg..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Turning JSON into Data Frame\n",
    "rackets = pd.read_json('CLdata.json')\n",
    "print(rackets.shape)\n",
    "rackets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jul 28</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Wilson BLX Blade Tour New</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul 28</td>\n",
       "      <td>125.0</td>\n",
       "      <td>Wilson BLX Blade Lite \"NEW\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jul 27</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Blade Wilson [K] Factor Tennis Racket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jul 30</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Wilson Tennis Rackets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jul 25</td>\n",
       "      <td>160.0</td>\n",
       "      <td>Wilson D200 RH Irons - 4 Iron to Pitching Wedg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     date  price                                              title\n",
       "0  Jul 28  100.0                          Wilson BLX Blade Tour New\n",
       "1  Jul 28  125.0                        Wilson BLX Blade Lite \"NEW\"\n",
       "2  Jul 27   45.0              Blade Wilson [K] Factor Tennis Racket\n",
       "3  Jul 30  110.0                              Wilson Tennis Rackets\n",
       "4  Jul 25  160.0  Wilson D200 RH Irons - 4 Iron to Pitching Wedg..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# I did not expect to have so little data. \n",
    "# Scrub the dollar sign away\n",
    "rackets.price = rackets.price.map(lambda x: None if x == None else int(re.sub('\\$', '', str(x))))\n",
    "rackets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      object\n",
       "price    float64\n",
       "title     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types\n",
    "rackets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rackets = rackets.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.92857142857142\n"
     ]
    }
   ],
   "source": [
    "average_price = rackets.price.mean()\n",
    "\n",
    "print(average_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average price is 131 dollars but according to the top results, the frames aren't that expensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17c3e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(rackets.price, bins=10)\n",
    "plt.xlabel('Rackets Price')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Price of Wilson Blades on SF Craigslist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two rackets where it sells for more than 400 dollars. That's why the average is so much higher. The median is probably around less than 100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Up\n",
    "\n",
    "Originally, I had tried to write this API for ebay but I ran into a lot of errors so I gave up eventually. Writing up what portion of the data I wanted was the hard part. I expected there to be more rackets on sale, but I ended up with a little data. The data exploration was good practice, but I checked the craigslist page again and saw that in the query there were some results that weren't actually tennis rackets so I guess I could have doen a better job scrubbing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
